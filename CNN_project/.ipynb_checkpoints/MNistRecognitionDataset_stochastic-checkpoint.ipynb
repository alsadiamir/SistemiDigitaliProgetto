{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simple CNN for MNIST hand-written digits image recognition\n",
    "#import the necessary libraries \n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERS\n",
    "blur_filter = np.array([[0.0625,0.125,0.0625],[0.125,0.250,0.125],[0.0625,0.125,0.0625]], dtype=np.float32)\n",
    "outline_filter = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]], dtype=np.float32)\n",
    "emboss_filter = np.array([[-2,-1,0],[-1,1,1],[0,1,2]], dtype=np.float32)\n",
    "\n",
    "#FILTER USED \n",
    "global used_filter\n",
    "used_filter = blur_filter\n",
    "\n",
    "#GLOBAL VARIABLES\n",
    "global DIM\n",
    "DIM = 8\n",
    "global FL\n",
    "FL = 4\n",
    "global IL\n",
    "IL = 8\n",
    "global e\n",
    "e = pow(2,-FL)\n",
    "global n_max\n",
    "n_max = pow(2,IL-1) - e\n",
    "global n_min\n",
    "n_min = -1*pow(2,IL-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELABORATION\n",
    "def elaboration(P):\n",
    "    N = P // e\n",
    "    p = (P - N*e) / e\n",
    "    rand = np.random.normal(0,1,1)\n",
    "    if rand <= p:\n",
    "        return N*e\n",
    "    else:\n",
    "        return (N+1)*e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERSION\n",
    "def conversion(Z):\n",
    "    if Z > n_max:\n",
    "        return n_max\n",
    "    elif Z < n_min:\n",
    "        return n_min\n",
    "    else:\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_compute(P):\n",
    "    P = P/pow(2,FL)\n",
    "    P = conversion( elaboration(P) )\n",
    "    pix = int(P)\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERSION\n",
    "def conv(image):\n",
    "    output = image.astype(int)\n",
    "    width = len(image)\n",
    "    height = len(image[0])\n",
    "    \n",
    "    for i in range(0,width):\n",
    "    #iterating in a column\n",
    "        for j in range(0,height):\n",
    "            #iterating in greyscale\n",
    "            output[i][j] = convert_and_compute(image[i][j])\n",
    "   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERNEL preprocessing\n",
    "def kernel_preprocessing(kernel):\n",
    "    kernel = np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]).reshape(3,3)\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            kernel[i][j] = conversion( elaboration(kernel[i][j]) )\n",
    "    \n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE preprocessing\n",
    "def image_preprocessing(image):\n",
    "    #shift of FL to left bits of the image pixels\n",
    "    npImage = np.array(image).astype(np.float32)\n",
    "    width = len(npImage)\n",
    "    height = len(npImage[0])\n",
    "\n",
    "    for i in range(0,width):\n",
    "    #iterating in a column\n",
    "        for j in range(0,height):\n",
    "            npImage[i][j] = npImage[i][j]*pow(2,FL)\n",
    "    \n",
    "    return npImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE postprocessing\n",
    "def image_postprocessing(image_convoluted):\n",
    "    return conv(image_convoluted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DStochastic(Layer):\n",
    "    def __init__(self, filters, kernel_size, kernel_initializer, **kwargs):\n",
    "        self.filters = filters\n",
    "        self.kernel_initializer=kernel_initializer\n",
    "        self.kernel_size = kernel_size\n",
    "        super(Conv2DStochastic, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def call(self):\n",
    "        #find out how to get the INPUTS from the PIPELINE\n",
    "        kernel_preprocessed = kernel_preprocessing(K.eval(self.kernel_initializer))\n",
    "        image_preprocessed = image_preprocessing(K.eval(model.input))\n",
    "        image_processed = K.conv2d(image_preprocessed, kernel_preprocessed)\n",
    "        return K.constant(image_postprocessing(image_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4233147610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_index = 258 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label \n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the imput elements\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3, 3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 43s 22ms/step - loss: 0.3445 - accuracy: 0.8953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f42336575b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680/1875 [=========================>....] - ETA: 4s - loss: 0.0856 - accuracy: 0.9741"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b4abef378313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#print(model.layers[0].get_weights()[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LAYER1\n",
    "for i in range(0, 10):    \n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    shape = np.shape(model.layers[0].get_weights()[0])\n",
    "    weights = weights.reshape(model.layers[0].get_weights()[0].size)\n",
    "    new_weights = np.zeros(shape=np.shape(weights))\n",
    "    for i in range(0,weights.size):\n",
    "        new_weights[i]= conversion( elaboration(weights[i]) )\n",
    "    new_weights = new_weights.reshape(shape)\n",
    "    \n",
    "    biases = model.layers[0].get_weights()[1]\n",
    "    shapeb = np.shape(model.layers[0].get_weights()[1])\n",
    "    biases = biases.reshape(model.layers[0].get_weights()[1].size)\n",
    "    new_biases = np.zeros(shape=np.shape(biases))\n",
    "    for i in range(0,biases.size):\n",
    "        new_biases[i]= conversion( elaboration(biases[i]) )\n",
    "    new_biases = new_biases.reshape(shapeb)    \n",
    "    \n",
    "    w_b = []\n",
    "    w_b.append(new_weights)\n",
    "    w_b.append(new_biases)\n",
    "    \n",
    "    model.layers[0].set_weights(w_b)  \n",
    "\n",
    "    #print(model.layers[0].get_weights()[0])\n",
    "    model.fit(x=x_train,y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 5960\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model100.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
