{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simple CNN for MNIST hand-written digits image recognition\n",
    "#import the necessary libraries \n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERS\n",
    "blur_filter = np.array([[0.0625,0.125,0.0625],[0.125,0.250,0.125],[0.0625,0.125,0.0625]], dtype=np.float32)\n",
    "outline_filter = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]], dtype=np.float32)\n",
    "emboss_filter = np.array([[-2,-1,0],[-1,1,1],[0,1,2]], dtype=np.float32)\n",
    "\n",
    "#FILTER USED \n",
    "global used_filter\n",
    "used_filter = blur_filter\n",
    "\n",
    "#GLOBAL VARIABLES\n",
    "global DIM\n",
    "DIM = 8\n",
    "global FL\n",
    "FL = 4\n",
    "global IL\n",
    "IL = 8\n",
    "global e\n",
    "e = pow(2,-FL)\n",
    "global n_max\n",
    "n_max = pow(2,IL-1) - e\n",
    "global n_min\n",
    "n_min = -1*pow(2,IL-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELABORATION\n",
    "def elaboration(P):\n",
    "    N = P // e\n",
    "    p = (P - N*e) / e\n",
    "    rand = np.random.normal(0,1,1)\n",
    "    if rand <= p:\n",
    "        return N*e\n",
    "    else:\n",
    "        return (N+1)*e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERSION\n",
    "def conversion(Z):\n",
    "    if Z > n_max:\n",
    "        return n_max\n",
    "    elif Z < n_min:\n",
    "        return n_min\n",
    "    else:\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_compute(P):\n",
    "    P = P/pow(2,FL)\n",
    "    P = conversion( elaboration(P) )\n",
    "    pix = int(P)\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERSION\n",
    "def conv(image):\n",
    "    output = image.astype(int)\n",
    "    width = len(image)\n",
    "    height = len(image[0])\n",
    "    \n",
    "    for i in range(0,width):\n",
    "    #iterating in a column\n",
    "        for j in range(0,height):\n",
    "            #iterating in greyscale\n",
    "            output[i][j] = convert_and_compute(image[i][j])\n",
    "   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERNEL preprocessing\n",
    "def kernel_preprocessing(kernel):\n",
    "    kernel = np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]).reshape(3,3)\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            kernel[i][j] = conversion( elaboration(kernel[i][j]) )\n",
    "    \n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE preprocessing\n",
    "def image_preprocessing(image):\n",
    "    #shift of FL to left bits of the image pixels\n",
    "    npImage = np.array(image).astype(np.float32)\n",
    "    width = len(npImage)\n",
    "    height = len(npImage[0])\n",
    "\n",
    "    for i in range(0,width):\n",
    "    #iterating in a column\n",
    "        for j in range(0,height):\n",
    "            npImage[i][j] = npImage[i][j]*pow(2,FL)\n",
    "    \n",
    "    return npImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE postprocessing\n",
    "def image_postprocessing(image_convoluted):\n",
    "    return conv(image_convoluted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DStochastic(Layer):\n",
    "    def __init__(self, filters, kernel_size, kernel_initializer, **kwargs):\n",
    "        self.filters = filters\n",
    "        self.kernel_initializer=kernel_initializer\n",
    "        self.kernel_size = kernel_size\n",
    "        super(Conv2DStochastic, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def call(self):\n",
    "        #find out how to get the INPUTS from the PIPELINE\n",
    "        kernel_preprocessed = kernel_preprocessing(K.eval(self.kernel_initializer))\n",
    "        image_preprocessed = image_preprocessing(K.eval(model.input))\n",
    "        image_processed = K.conv2d(image_preprocessed, kernel_preprocessed)\n",
    "        return K.constant(image_postprocessing(image_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef3849e850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_index = 258 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label \n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the imput elements\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3, 3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 37s 19ms/step - loss: 0.3653 - accuracy: 0.8904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fef389b54f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0841 - accuracy: 0.9736\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0592 - accuracy: 0.9812\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0449 - accuracy: 0.9857\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0369 - accuracy: 0.9878\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0300 - accuracy: 0.9901\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0262 - accuracy: 0.9913\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0240 - accuracy: 0.9913\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0201 - accuracy: 0.9931\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0204 - accuracy: 0.9930\n",
      "1713/1875 [==========================>...] - ETA: 1s - loss: 0.0169 - accuracy: 0.9943"
     ]
    }
   ],
   "source": [
    "#LAYER1\n",
    "for i in range(0, 100):    \n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    shape = np.shape(model.layers[0].get_weights()[0])\n",
    "    weights = weights.reshape(model.layers[0].get_weights()[0].size)\n",
    "    new_weights = np.zeros(shape=np.shape(weights))\n",
    "    for i in range(0,weights.size):\n",
    "        new_weights[i]= conversion( elaboration(weights[i]) )\n",
    "    new_weights = new_weights.reshape(shape)\n",
    "    \n",
    "    biases = model.layers[0].get_weights()[1]\n",
    "    shapeb = np.shape(model.layers[0].get_weights()[1])\n",
    "    biases = biases.reshape(model.layers[0].get_weights()[1].size)\n",
    "    new_biases = np.zeros(shape=np.shape(biases))\n",
    "    for i in range(0,biases.size):\n",
    "        new_biases[i]= conversion( elaboration(biases[i]) )\n",
    "    new_biases = new_biases.reshape(shapeb)    \n",
    "    \n",
    "    w_b = []\n",
    "    w_b.append(new_weights)\n",
    "    w_b.append(new_biases)\n",
    "    \n",
    "    model.layers[0].set_weights(w_b)  \n",
    "\n",
    "    #print(model.layers[0].get_weights()[0])\n",
    "    model.fit(x=x_train,y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 5960\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model100.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
