{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simple CNN for MNIST hand-written digits image recognition\n",
    "#import the necessary libraries \n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERS\n",
    "blur_filter = np.array([[0.0625,0.125,0.0625],[0.125,0.250,0.125],[0.0625,0.125,0.0625]], dtype=np.float32)\n",
    "outline_filter = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]], dtype=np.float32)\n",
    "emboss_filter = np.array([[-2,-1,0],[-1,1,1],[0,1,2]], dtype=np.float32)\n",
    "\n",
    "#FILTER USED \n",
    "global used_filter\n",
    "used_filter = blur_filter\n",
    "\n",
    "#GLOBAL VARIABLES\n",
    "global DIM\n",
    "DIM = 8\n",
    "global FL\n",
    "FL = 4\n",
    "global IL\n",
    "IL = 8\n",
    "global e\n",
    "e = pow(2,-FL)\n",
    "global n_max\n",
    "n_max = pow(2,IL-1) - e\n",
    "global n_min\n",
    "n_min = -1*pow(2,IL-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELABORATION\n",
    "def elaboration(P):\n",
    "    N = P // e\n",
    "    p = (P - N*e) / e\n",
    "    rand = np.random.normal(0,1,1)\n",
    "    if rand <= p:\n",
    "        return N*e\n",
    "    else:\n",
    "        return (N+1)*e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERSION\n",
    "def conversion(Z):\n",
    "    if Z > n_max:\n",
    "        return n_max\n",
    "    elif Z < n_min:\n",
    "        return n_min\n",
    "    else:\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_compute(P):\n",
    "    P = P/pow(2,FL)\n",
    "    P = conversion( elaboration(P) )\n",
    "    pix = int(P)\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERSION\n",
    "def conv(image):\n",
    "    output = image.astype(int)\n",
    "    width = len(image)\n",
    "    height = len(image[0])\n",
    "    \n",
    "    for i in range(0,width):\n",
    "    #iterating in a column\n",
    "        for j in range(0,height):\n",
    "            #iterating in greyscale\n",
    "            output[i][j] = convert_and_compute(image[i][j])\n",
    "   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERNEL preprocessing\n",
    "def kernel_preprocessing(kernel):\n",
    "    kernel = np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]).reshape(3,3)\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            kernel[i][j] = conversion( elaboration(kernel[i][j]) )\n",
    "    \n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE preprocessing\n",
    "def image_preprocessing(image):\n",
    "    #shift of FL to left bits of the image pixels\n",
    "    npImage = np.array(image).astype(np.float32)\n",
    "    width = len(npImage)\n",
    "    height = len(npImage[0])\n",
    "\n",
    "    for i in range(0,width):\n",
    "    #iterating in a column\n",
    "        for j in range(0,height):\n",
    "            npImage[i][j] = npImage[i][j]*pow(2,FL)\n",
    "    \n",
    "    return npImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE postprocessing\n",
    "def image_postprocessing(image_convoluted):\n",
    "    return conv(image_convoluted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DStochastic(Layer):\n",
    "    def __init__(self, filters, kernel_size, kernel_initializer, **kwargs):\n",
    "        self.filters = filters\n",
    "        self.kernel_initializer=kernel_initializer\n",
    "        self.kernel_size = kernel_size\n",
    "        super(Conv2DStochastic, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def call(self):\n",
    "        #find out how to get the INPUTS from the PIPELINE\n",
    "        kernel_preprocessed = kernel_preprocessing(K.eval(self.kernel_initializer))\n",
    "        image_preprocessed = image_preprocessing(K.eval(model.input))\n",
    "        image_processed = K.conv2d(image_preprocessed, kernel_preprocessed)\n",
    "        return K.constant(image_postprocessing(image_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd70fb8f610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_index = 258 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label \n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the imput elements\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3, 3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.3810 - accuracy: 0.8877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd71009fa30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0882 - accuracy: 0.9726\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0647 - accuracy: 0.9798\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0480 - accuracy: 0.9841\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0384 - accuracy: 0.9876\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0325 - accuracy: 0.9891\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0279 - accuracy: 0.9907\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0239 - accuracy: 0.9921\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0261 - accuracy: 0.9918\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0201 - accuracy: 0.9934\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0217 - accuracy: 0.9928\n"
     ]
    }
   ],
   "source": [
    "#LAYER1\n",
    "for i in range(0, 10):    \n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    shape = np.shape(model.layers[0].get_weights()[0])\n",
    "    weights = weights.reshape(model.layers[0].get_weights()[0].size)\n",
    "    new_weights = np.zeros(shape=np.shape(weights))\n",
    "    for i in range(0,weights.size):\n",
    "        new_weights[i]= conversion( elaboration(weights[i]) )\n",
    "    new_weights = new_weights.reshape(shape)\n",
    "    \n",
    "    biases = model.layers[0].get_weights()[1]\n",
    "    shapeb = np.shape(model.layers[0].get_weights()[1])\n",
    "    biases = biases.reshape(model.layers[0].get_weights()[1].size)\n",
    "    new_biases = np.zeros(shape=np.shape(biases))\n",
    "    for i in range(0,biases.size):\n",
    "        new_biases[i]= conversion( elaboration(biases[i]) )\n",
    "    new_biases = new_biases.reshape(shapeb)    \n",
    "    \n",
    "    w_b = []\n",
    "    w_b.append(new_weights)\n",
    "    w_b.append(new_biases)\n",
    "    \n",
    "    model.layers[0].set_weights(w_b)  \n",
    "\n",
    "    #print(model.layers[0].get_weights()[0])\n",
    "    model.fit(x=x_train,y=y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0674 - accuracy: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06735769659280777, 0.9843999743461609]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN9klEQVR4nO3db4xU9b3H8c9XWkRpIyAbJGCktyEmeMmldYJNII03za1ISLCamPKg4SaY7QNJwFRzTXlQgiH+iW2VaGqokCLptSFpER7IvXBJo+GByGq4C6hXuWZJWdhliDFYIbbAtw/20Gxhz2/WOWfmzO73/Uo2M3O+85v5ZvTDOXN+M/MzdxeA8e+6qhsA0B6EHQiCsANBEHYgCMIOBPGVdj7Z9OnTfc6cOe18SiCUvr4+nT171kaqFQq7mS2R9LykCZJedvenUvefM2eOenp6ijwlgIRarZZba/ow3swmSHpR0r2S5klaYWbzmn08AK1V5D37QknH3f1jd/+LpN9JWl5OWwDKViTssyT9adjtk9m2f2Bm3WbWY2Y99Xq9wNMBKKLlZ+PdfbO719y91tXV1eqnA5CjSNj7Jd067PbsbBuADlQk7IckzTWzb5jZREk/lLS7nLYAlK3pqTd3v2hmqyX9t4am3ra6+7HSOgNQqkLz7O7+uqTXS+oFQAvxcVkgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmjrks1ojcHBwdzarl27kmM3btyYrE+aNClZP3jwYLI+ZcqUZB3tw54dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Jgnr0DnDp1Klmv1WrJer1ez61dunSpqZ6ucPdkvaurK1l/9NFHc2uPPfZYcuy0adOSdXw5hcJuZn2SPpN0SdJFd0//XwmgMmXs2f/V3c+W8DgAWoj37EAQRcPukvaa2Ttm1j3SHcys28x6zKwn9d4SQGsVDftid/+2pHslPWxm3736Du6+2d1r7l5rdDIHQOsUCru792eXZyTtlLSwjKYAlK/psJvZZDP7+pXrkr4v6WhZjQEoV5Gz8TMk7TSzK4/zn+7+X6V0Fczzzz+frA8MDLSpk2tl/31zNZrHf/rpp3NrmzZtSo6dMGFCsr5nz55kfdGiRcl6NE2H3d0/lvQvJfYCoIWYegOCIOxAEIQdCIKwA0EQdiAIvuLaBvv27UvWn3322TZ10lkuXLhQaPzatWuT9UOHDhV6/PGGPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8exucP38+WW/0c81F7Ny5M1k/d+5csr5y5coy2ynV4cOHk/UDBw7k1hYvXlxyN52PPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME8+zjw3HPP5daWLVuWHHv58uVkvegqPnPnzs2tbdiwITl2+/btyXqjn7F+5JFHcmsRv+vOnh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCefRxYsWJFbq3RsseN6kuWLGmqp9F48sknk/VG8+yN1Ov1QuPHm4Z7djPbamZnzOzosG3TzGyfmX2UXU5tbZsAihrNYfxvJF39z/vjkva7+1xJ+7PbADpYw7C7+5uSPrlq83JJ27Lr2yTdV25bAMrW7Am6Ge5+Ors+IGlG3h3NrNvMesysh/dQQHUKn433oV9LzP3FRHff7O41d68V/VIFgOY1G/ZBM5spSdnlmfJaAtAKzYZ9t6QrvzG8UtKuctoB0CoN59nN7FVJd0uabmYnJf1M0lOSdpjZKkknJD3YyiaRtm7dutxad3d3occ+fvx4sr5nz55kfeLEibm1t956q6me0JyGYXf3vE9sfK/kXgC0EB+XBYIg7EAQhB0IgrADQRB2IAi+4joOvPzyy03VEAt7diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Ignn2Npg1a1ayfv311yfrX3zxRZntjBs33HBDsv7CCy+0qZOxgT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPHsb1Gq1ZP2ZZ55J1tesWVNmO+PG7Nmzk/Vly5a1qZOxgT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPHsH2L9/f9UttMzkyZNza/PmzUuOPXToULLe39+frB85ciS3Nn/+/OTY8ajhnt3MtprZGTM7OmzbejPrN7PD2d/S1rYJoKjRHMb/RtKSEbb/0t0XZH+vl9sWgLI1DLu7vynpkzb0AqCFipygW21mvdlh/tS8O5lZt5n1mFlPvV4v8HQAimg27L+S9E1JCySdlvTzvDu6+2Z3r7l7raurq8mnA1BUU2F390F3v+TulyX9WtLCctsCULamwm5mM4fd/IGko3n3BdAZGs6zm9mrku6WNN3MTkr6maS7zWyBJJfUJ+nHrWtx7Ltw4UKyfvDgwTZ1cq077rgjWe/r60vWP//882R94cL8g77t27cnxzb6vvr58+eT9SeeeCK3tmPHjuTY8ahh2N19xQibt7SgFwAtxMdlgSAIOxAEYQeCIOxAEIQdCIKvuLbB3r17k/XBwcFCj3/TTTfl1rZsSU+cLF2a/sLiG2+8kaw/8MADyXpvb29u7dixY8mxRb399tstffyxhj07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPPs4sGLFSF9MHHL//fcXeux77rknWW80l71q1arc2vr165tpadQeeuihlj7+WMOeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJ59HEj9pPKlS5eSYydMmJCsX7x4MVmfNGlSsj4wMJBbO3HiRHJsUbfffntLH3+sYc8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewz94Gd911V7I+ZcqUZP3TTz9N1l955ZXc2o033pgce/PNNyfrJ0+eTNa3bduWrLfSddel91UffvhhmzoZGxru2c3sVjP7o5m9Z2bHzGxNtn2ame0zs4+yy6mtbxdAs0ZzGH9R0k/cfZ6k70h62MzmSXpc0n53nytpf3YbQIdqGHZ3P+3u72bXP5P0vqRZkpZLunIMt03SfS3qEUAJvtQJOjObI+lbkg5KmuHup7PSgKQZOWO6zazHzHrq9XqRXgEUMOqwm9nXJP1e0lp3Pze85u4uyUca5+6b3b3m7rWurq5CzQJo3qjCbmZf1VDQf+vuf8g2D5rZzKw+U9KZ1rQIoAwNp97MzCRtkfS+u/9iWGm3pJWSnsoud7Wkw3HglltuSdZXr16drG/cuDFZHzqwGtlLL72UHDuWzZ8/P1lft25dmzoZG0Yzz75I0o8kHTGzw9m2n2oo5DvMbJWkE5IebEmHAErRMOzufkCS5ZS/V247AFqFj8sCQRB2IAjCDgRB2IEgCDsQBF9x7QAbNmxI1l977bVk/YMPPsitNfop6CoNfYQj38SJE5P1TZs2ldnOuMeeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJ59DOjt7U3WBwcHc2t33nlncuypU6ea6mm0brvtttzaiy++mBy7dOnSstsJjT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPPs4MGPGiCtvSWq85DLiYM8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0E0DLuZ3WpmfzSz98zsmJmtybavN7N+Mzuc/fHlY6CDjeZDNRcl/cTd3zWzr0t6x8z2ZbVfuvuzrWsPQFlGsz77aUmns+ufmdn7kma1ujEA5fpS79nNbI6kb0k6mG1abWa9ZrbVzKbmjOk2sx4z66nX68W6BdC0UYfdzL4m6feS1rr7OUm/kvRNSQs0tOf/+Ujj3H2zu9fcvdbV1VW8YwBNGVXYzeyrGgr6b939D5Lk7oPufsndL0v6taSFrWsTQFGjORtvkrZIet/dfzFs+8xhd/uBpKPltwegLKM5G79I0o8kHTGzw9m2n0paYWYLJLmkPkk/bkF/AEoymrPxBySNtJD26+W3A6BV+AQdEARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCHP39j2ZWV3SiWGbpks627YGvpxO7a1T+5LorVll9nabu4/4+29tDfs1T27W4+61yhpI6NTeOrUvid6a1a7eOIwHgiDsQBBVh31zxc+f0qm9dWpfEr01qy29VfqeHUD7VL1nB9AmhB0IopKwm9kSM/s/MztuZo9X0UMeM+szsyPZMtQ9Ffey1czOmNnRYdummdk+M/souxxxjb2KeuuIZbwTy4xX+tpVvfx529+zm9kESR9K+jdJJyUdkrTC3d9rayM5zKxPUs3dK/8Ahpl9V9KfJb3i7v+cbXtG0ifu/lT2D+VUd/+PDultvaQ/V72Md7Za0czhy4xLuk/Sv6vC1y7R14Nqw+tWxZ59oaTj7v6xu/9F0u8kLa+gj47n7m9K+uSqzcslbcuub9PQ/yxtl9NbR3D30+7+bnb9M0lXlhmv9LVL9NUWVYR9lqQ/Dbt9Up213rtL2mtm75hZd9XNjGCGu5/Org9ImlFlMyNouIx3O121zHjHvHbNLH9eFCforrXY3b8t6V5JD2eHqx3Jh96DddLc6aiW8W6XEZYZ/7sqX7tmlz8vqoqw90u6ddjt2dm2juDu/dnlGUk71XlLUQ9eWUE3uzxTcT9/10nLeI+0zLg64LWrcvnzKsJ+SNJcM/uGmU2U9ENJuyvo4xpmNjk7cSIzmyzp++q8pah3S1qZXV8paVeFvfyDTlnGO2+ZcVX82lW+/Lm7t/1P0lINnZH/f0nrqughp69/kvS/2d+xqnuT9KqGDuv+qqFzG6sk3Sxpv6SPJP2PpGkd1Nt2SUck9WooWDMr6m2xhg7ReyUdzv6WVv3aJfpqy+vGx2WBIDhBBwRB2IEgCDsQBGEHgiDsQBCEHQiCsANB/A0iGjTVMIkvJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 5960\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx6gsbp3_/assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
